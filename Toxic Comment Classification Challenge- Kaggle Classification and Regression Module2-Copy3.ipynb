{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n*****************************************************************************************************************************************\\n*****************************************************************************************************************************************\\n* Author             : Anjana Tiha\\n* Author Details     : Masters of Science, Computer Science, University of Memphis, Memphis, Tennessee, USA (May 2018)\\n*****************************************************************************************************************************************\\n*****************************************************************************************************************************************\\n* Project Name       : Toxic Comment Classification Challenge - Kaggle Classification and Regression Module\\n* Description        : Machine Learning (Supervised Learning/ Classification/ Predictive Algorithm) for identifying toxic comments.\\n* Solution           : Using TFIDF Vector and sentence, word and charcacter level analysis following a recent 2018 AAAI conference paper.\\n* Input              : Collection of 159000 comments\\n* Output             : classifying toxic/hateful comment\\n* Start Date         : 07.04.2018\\n* Last Update        : \\n* Tools Requirement  : Anaconda/PyCharm, Python\\n* Comments           : Please use Anaconda editor for visualization and convenience.\\n* Version History    : 1.0.0.0\\n* Current Version    : 1.0.0.0\\n*****************************************************************************************************************************************\\n*****************************************************************************************************************************************\\n\\n'"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "*****************************************************************************************************************************************\n",
    "*****************************************************************************************************************************************\n",
    "* Author             : Anjana Tiha\n",
    "* Author Details     : Masters of Science, Computer Science, University of Memphis, Memphis, Tennessee, USA (May 2018)\n",
    "*****************************************************************************************************************************************\n",
    "*****************************************************************************************************************************************\n",
    "* Project Name       : Toxic Comment Classification Challenge - Kaggle Classification and Regression Module\n",
    "* Description        : Machine Learning (Supervised Learning/ Classification/ Predictive Algorithm) for identifying toxic comments.\n",
    "* Solution           : Using TFIDF Vector and sentence, word and charcacter level analysis following a recent 2018 AAAI conference paper.\n",
    "* Input              : Collection of 159000 comments\n",
    "* Output             : classifying toxic/hateful comment\n",
    "* Start Date         : 07.04.2018\n",
    "* Last Update        : \n",
    "* Tools Requirement  : Anaconda/PyCharm, Python\n",
    "* Comments           : Please use Anaconda editor for visualization and convenience.\n",
    "* Version History    : 1.0.0.0\n",
    "* Current Version    : 1.0.0.0\n",
    "*****************************************************************************************************************************************\n",
    "*****************************************************************************************************************************************\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFollowed the following paper:\\nPaper Summray:\\nTitle:    Anatomy of Online Hate: Developing a Taxonomy and Machine Learning \\n          Models for Identifying and Classifying Hate in Online News Media\\nAuthors:  Joni Salminen,*†§ Hind Almerekhi,*Milica Milenković, Soon-gyo Jung,*Jisun An,*Haewoon Kwak,*,Bernard J. Jansen*\\n          Qatar Computing Research Institute, Hamad Bin Khalifa University †\\n          Turku School of Economics at the University of Turku\\n          Independent Researcher\\n'"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Followed the following paper:\n",
    "Paper Summray:\n",
    "Title:    Anatomy of Online Hate: Developing a Taxonomy and Machine Learning \n",
    "          Models for Identifying and Classifying Hate in Online News Media\n",
    "Authors:  Joni Salminen,*†§ Hind Almerekhi,*Milica Milenković, Soon-gyo Jung,*Jisun An,*Haewoon Kwak,*,Bernard J. Jansen*\n",
    "          Qatar Computing Research Institute, Hamad Bin Khalifa University †\n",
    "          Turku School of Economics at the University of Turku\n",
    "          Independent Researcher\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from string import punctuation\n",
    "import re\n",
    "from tokenize import tokenize\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class File:\n",
    "    def __init__(self, train_X_file, train_Y_file, test_X_file, test_Y_file):\n",
    "        if train_X_file:\n",
    "            self.train_X = pd.read_csv(train_X_file)\n",
    "        if train_Y_file:\n",
    "            self.train_Y = pd.read_csv(train_Y_file)\n",
    "        else:\n",
    "            self.train_Y = None\n",
    "        if test_X_file:\n",
    "            self.test_X = pd.read_csv(test_X_file)\n",
    "        if test_Y_file:\n",
    "            self.test_Y = pd.read_csv(test_Y_file)\n",
    "            \n",
    "    def get_content(self):\n",
    "        return self.train_X, self.train_Y, self.test_X, self.test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, train_X, train_Y, test_X, test_Y, train_test, train_XY, test_XY, targets):\n",
    "        self.train_X = train_X\n",
    "        self.test_X = test_X\n",
    "     \n",
    "        self.train_Y = pd.DataFrame()\n",
    "        self.test_Y = pd.DataFrame()\n",
    "        \n",
    "        self.feature_cols = None\n",
    "        \n",
    "        if train_XY:\n",
    "            for col in targets:\n",
    "                if self.train_Y.empty:\n",
    "                    self.train_Y = pd.DataFrame(data=train_X[col], columns=[col])\n",
    "                else:\n",
    "                    self.train_Y = pd.concat([self.train_Y, train_X[[col]]], axis=1)\n",
    "                    \n",
    "                self.train_X.drop(columns=col, inplace=True)\n",
    "        else:\n",
    "            self.train_Y = train_Y\n",
    "            \n",
    "        if test_XY:\n",
    "            for col in targets:\n",
    "                if self.test_Y.empty:\n",
    "                    self.test_Y = pd.DataFrame(data=test_X[col], columns=[col])\n",
    "                else:\n",
    "                    self.test_Y = pd.concat([self.test_Y, test_X[[col]]], axis=1)\n",
    "                    \n",
    "                    \n",
    "                self.test_X.drop(columns=col, inplace=True)\n",
    "        else:\n",
    "            self.test_Y = test_Y\n",
    "\n",
    "    def set_feature_cols(self, feature_cols):\n",
    "        if self.feature_cols:\n",
    "            self.feature_cols.append(feature_cols)\n",
    "        else: self.feature_cols = feature_cols\n",
    "    def remove_feature_cols(self, feature_cols):\n",
    "        if self.feature_cols:\n",
    "            self.feature_cols.remove(feature_cols)\n",
    "        else: self.feature_cols = None\n",
    "    \n",
    "    def fill_columns_selected(self, columns_names, columns_val, inplace=True):\n",
    "        for cols in columns_names:\n",
    "            self.train_X[cols].fillna(columns_val, inplace=inpl)\n",
    "            self.test_X[cols].fillna(columns_val, inplace=inpl)\n",
    "            \n",
    "    def fill_columns(self, val, inpl=True):\n",
    "        self.train_X.fillna(val, inplace=inpl)\n",
    "        self.test_X.fillna(val, inplace=inpl)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.train_X, self.train_Y, self.test_X, self.test_Y\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split:\n",
    "    def __init__(self, splitter_name, n_splits, test_size, random_state):\n",
    "        if splitter_name == 'KFold':\n",
    "            self.splitter = KFold(n_splits, random_state, shuffle)\n",
    "        elif splitter_name == 'StratifiedShuffleSplit':\n",
    "            self.splitter = StratifiedShuffleSplit(n_splits, test_size, random_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, model_type):\n",
    "        if model_type == 'Classification':\n",
    "            self.models = {\n",
    "                \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "                \"BernoulliNB\": BernoulliNB(),\n",
    "            #     \"BernoulliRBM\": BernoulliRBM(),\n",
    "                \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "                \"ExtraTreesClassifier\": ExtraTreesClassifier(),\n",
    "            #     \"GaussianMixture\": GaussianMixture(),\n",
    "            #     \"GaussianNB\": GaussianNB(),\n",
    "            #     \"GaussianProcessClassifier\": GaussianProcessClassifier(),\n",
    "                \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "            #     \"KDTree\": KDTree(),\n",
    "            #     \"KNeighborsClassifier\": KNeighborsClassifier(3),\n",
    "                \"LogisticRegression\": LogisticRegression(),\n",
    "                \"LinearSVC\": LinearSVC(),\n",
    "                \"MLPClassifier\": MLPClassifier(),\n",
    "                \"MultinomialNB\": MultinomialNB(),\n",
    "            #     \"NearestNeighbors\": NearestNeighbors(),\n",
    "            #     \"NuSVC\": NuSVC(),\n",
    "                \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "                \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "                \"SVC Linear\": SVC(kernel=\"linear\", C=0.025),\n",
    "                \"SVC\": SVC(),\n",
    "                \"SVC Gamma\": SVC(gamma=2, C=1)\n",
    "            #     VotingClassifier: VotingClassifier(),\n",
    "            }\n",
    "        elif model_type == 'Regression':\n",
    "            self.models  = {\n",
    "                \"AdaBoostRegressor\": AdaBoostRegressor(),\n",
    "            #     \"ARDRegression\": ARDRegression(),\n",
    "                \"BaggingRegressor\": BaggingRegressor(),\n",
    "            #     \"BernoulliRBM\": BernoulliRBM(),\n",
    "                \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n",
    "                \"ExtraTreesRegressor\": ExtraTreesRegressor(),\n",
    "                \"ExtraTreeRegressor\": ExtraTreeRegressor(),\n",
    "            #     \"GaussianMixture\": GaussianMixture(),\n",
    "            #     \"GaussianNB\": GaussianNB(),\n",
    "                \"GaussianProcessRegressor\": GaussianProcessRegressor(),\n",
    "                \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n",
    "                \"HuberRegressor\": HuberRegressor(),\n",
    "            #     \"IsotonicRegression\": IsotonicRegression(),\n",
    "                \"KernelRidge\": KernelRidge(),\n",
    "            #     \"KDTree\": KDTree(),\n",
    "            #     \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "            #     \"LinearRegression\": LinearRegression(), \n",
    "                \"LogisticRegression\": LogisticRegression(),\n",
    "                \"LogisticRegressionCV\": LogisticRegressionCV(),\n",
    "            #     \"logistic_regression_path\": logistic_regression_path(),\n",
    "                \"LinearSVR\": LinearSVR(),\n",
    "                \"MLPRegressor\": MLPRegressor(),\n",
    "            #     \"MultinomialNB\": MultinomialNB(),\n",
    "                \"NuSVR\": NuSVR(),\n",
    "                \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(),\n",
    "            #     \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "                \"RadiusNeighborsRegressor\": RadiusNeighborsRegressor(),\n",
    "                \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "                \"RandomizedLogisticRegression\": RandomizedLogisticRegression(),\n",
    "                \"RANSACRegressor\": RANSACRegressor(),\n",
    "                \"SGDRegressor\": SGDRegressor(),\n",
    "                \"SVR\": SVR(),\n",
    "                \"TheilSenRegressor\": TheilSenRegressor(),\n",
    "            }\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_file =r'C:\\Users\\Anjana Tiha\\Drive D\\Programming\\Projects\\Toxic Comment Classification Challenge\\Toxic Comment Classification Challenge\\all\\train.csv'\n",
    "test_X_file = r'C:\\Users\\Anjana Tiha\\Drive D\\Programming\\Projects\\Toxic Comment Classification Challenge\\Toxic Comment Classification Challenge\\all\\test.csv'\n",
    "test_Y_file = r'C:\\Users\\Anjana Tiha\\Drive D\\Programming\\Projects\\Toxic Comment Classification Challenge\\Toxic Comment Classification Challenge\\all\\test_labels.csv'\n",
    "\n",
    "file_obj = File(train_X_file, None, test_X_file, test_Y_file)\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = file_obj.get_content()\n",
    "\n",
    "\n",
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "data_obj = Data(train_X, None, test_X, test_Y, False, True, False, targets)\n",
    "\n",
    "data_obj.train_Y['none'] = 1-data_obj.train_Y[targets].max(axis=1)\n",
    "data_obj.test_Y['none'] = 1-data_obj.test_Y[targets].max(axis=1)\n",
    "data_obj.train_Y['any'] = data_obj.train_Y[targets].max(axis=1)\n",
    "data_obj.test_Y['any'] = data_obj.test_Y[targets].max(axis=1)\n",
    "data_obj.fill_columns(\"unknown\", True)\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = data_obj.get_data()\n",
    "\n",
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'none', 'any']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def clean_text(text, alpha=True, punc=False, case_active=False, remove_stopwords=True):\n",
    "        if alpha:\n",
    "            text  = re.sub(r\"[^a-z]\", \" \", text)\n",
    "        if case_active==False:\n",
    "            text  = text .lower()\n",
    "        if punc==False:\n",
    "            text  = ''.join([c for c in text if c not in punctuation])\n",
    "        if remove_stopwords:\n",
    "            cached_stopwords = stopwords.words(\"english\")\n",
    "            self.text = ' '.join([word for word in text.split() if word not in cached_stopwords])\n",
    "        text = re.sub(r\" +\", \" \", text)\n",
    "        text = text.strip()\n",
    "        text = text .split()\n",
    "        return text   \n",
    "\n",
    "    def tokenize(text, alpha=True, punc=False, case_active=False, remove_stopwords=True):\n",
    "        if alpha:\n",
    "            text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "        if case_active==False:\n",
    "            text = text.lower()\n",
    "        if punc==False:\n",
    "            text = ''.join([c for c in text if c not in punctuation])\n",
    "        if remove_stopwords:\n",
    "            cached_stopwords = stopwords.words(\"english\")\n",
    "            text = ' '.join([word for word in text.split() if word not in cached_stopwords])\n",
    "        text = re.sub(r\" +\", \" \", text)\n",
    "        text = text.strip()\n",
    "        text = text.split()\n",
    "        return text \n",
    "\n",
    "\n",
    "    def single_char_cnt(text, alpha=False, punc=False, remove_stopwords=True):\n",
    "        text = text\n",
    "        if alpha:\n",
    "            text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "        if punc==False:\n",
    "            text = ''.join([c for c in text if c not in punctuation])\n",
    "        if remove_stopwords:\n",
    "            cached_stopwords = stopwords.words(\"english\")\n",
    "            text = ' '.join([word for word in text.split() if word not in cached_stopwords])\n",
    "\n",
    "        text = re.sub(r\" +\", \" \", text)\n",
    "        text = text.strip()\n",
    "        text = text.split()\n",
    "\n",
    "        c=0\n",
    "        for tok in text:\n",
    "            if len(tok.strip())==1: c+=1\n",
    "        return  c\n",
    "\n",
    "    def find_urls(text):\n",
    "        return re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text)\n",
    "\n",
    "\n",
    "    def count_modals(text):\n",
    "        modals = ['can', 'could', 'may', 'might', 'must', 'will', 'would', 'should']\n",
    "\n",
    "        toks = text.split(' ')\n",
    "\n",
    "        c=0\n",
    "        for tok in toks:\n",
    "            if tok in modals: c+=1\n",
    "        return c\n",
    "\n",
    "    def non_alpha_mid(text, alpha=False, punc=False, remove_stopwords=True):\n",
    "        text = re.sub(r\" +\", \" \", text)\n",
    "        if punc==False:\n",
    "            text = ''.join([c for c in text if c not in punctuation])\n",
    "        if remove_stopwords:\n",
    "            cached_stopwords = stopwords.words(\"english\")\n",
    "            text = ''.join([word for word in text if word not in cached_stopwords])\n",
    "\n",
    "        text = text.split()\n",
    "\n",
    "        c=0\n",
    "        for tok in text:\n",
    "            m=0\n",
    "            for ch in tok:\n",
    "                if ch.isalpha()==0: m+=1\n",
    "            if (m>1 and len(tok)>1) or (m>=1): c+=1\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features:    \n",
    "    def __init__(self, train_X, test_X, columns_text):\n",
    "        self.train_X = train_X\n",
    "        self.test_X = test_X\n",
    "        self.columns_text = columns_text\n",
    "        \n",
    "        self.train_X_features = []\n",
    "        self.test_X_features = []\n",
    "        \n",
    "    def get_features_X(self, X, features):\n",
    "        count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "        prep_obj = Preprocess()\n",
    "        print(X)\n",
    "        try:\n",
    "            for col in self.columns_text:\n",
    "                length=X[col].size\n",
    "                \n",
    "                i=0\n",
    "                features_col = []\n",
    "                for line in range(0, length-1):\n",
    "                    counts = []\n",
    "                    text = X[col][line]\n",
    "                    \n",
    "                    counts.append(text.count(\"!\"))\n",
    "                    \n",
    "                    counts.append(text.count(\"?\"))\n",
    "                    counts.append(text.count(\".\"))\n",
    "                    counts.append(count(text,set(string.punctuation)))\n",
    "                    counts.append(len(re.findall('[''\"\"]', text)))\n",
    "                    \n",
    "                    counts.append(single_char_cnt(text, alpha=False, punc=False, remove_stopwords=False))\n",
    "                    counts.append(len(find_urls(text)))\n",
    "                    counts.append(len(text))\n",
    "                    counts.append(len(text.split()))\n",
    "                    counts.append(sum(1 for c in text if c.isupper()))\n",
    "                    counts.append(count_modals(text))\n",
    "                    counts.append(len(re.findall(r'[\\U0001f600-\\U0001f650]', text)))\n",
    "                    counts.append(non_alpha_mid(text, alpha=False, punc=False, remove_stopwords=False))\n",
    "                    print(count)\n",
    "                    features_col.append(counts)\n",
    "                features.append(features_col)\n",
    "        except:\n",
    "            print(\"Error:\", line)\n",
    "        \n",
    "        features = np.array(features)\n",
    "                \n",
    "        return features\n",
    "            \n",
    "    def get_features(self):\n",
    "        self.train_X_features = get_features_X(self.train_X, self.columns_text)\n",
    "        self.test_X_features = get_features_X(self.test_X, self.columns_text)\n",
    "        \n",
    "        return self.train_X_features, self.test_X_features \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_features_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-403-727f4d16b927>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfeatures_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_X_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-402-0195874c62db>\u001b[0m in \u001b[0;36mget_features\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_X_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_features_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_X_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_features_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_features_X' is not defined"
     ]
    }
   ],
   "source": [
    "text_columns = ['comment_text']\n",
    "features_obj = Features(train_X, test_X, text_columns)\n",
    "train_X_features, test_X_features = features_obj.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(tokenizer=clean_text, min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1, smooth_idf=1 )\n",
    "\n",
    "# # tfidf = TfidfVectorizer(tokenizer=clean_text, min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1, smooth_idf=1 )\n",
    "# # tfidf = TfidfVectorizer(decode_error='strict', strip_accents='unicode', lowercase=True, preprocessor=preprocess, tokenizer=tokenize, analyzer='word', stop_words='english', ngram_range=(1, 1), max_df=1.0, min_df=1, norm='l2', use_idf=True, smooth_idf=True)\n",
    "\n",
    "# # tfidf = TfidfVectorizer(decode_error='strict', strip_accents='unicode', lowercase=True, preprocessor=preprocess, tokenizer=tokenize, analyzer='word', stop_words='english', max_df=0.9, min_df=3, norm='l2', use_idf=True, smooth_idf=True)\n",
    "\n",
    "# x = tfidf.fit_transform(train['comment_text'])\n",
    "# test_x = tfidf.transform(test['comment_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # print(data.isnull().sum(axis=1))\n",
    "# # X = features.values\n",
    "# # Y = target\n",
    "# # X = x+test_x\n",
    "# X = x\n",
    "# # Y = train['comment_text'] + test['comment_text']\n",
    "# # Y = train['comment_text']\n",
    "\n",
    "# # # print(features.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def model_evaluation(X, Y, X_test, y_test, splitter, model, report, details):\n",
    "    accuracy = 0\n",
    "    f1 = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    i=0\n",
    "    if report:\n",
    "        print(\"*\"*50, \" START \", \"*\"*50)\n",
    "        print(\"Model Description:\")\n",
    "        print(model)\n",
    "        print(\"-\"*100,\"\\n\")\n",
    "      \n",
    "    if splitter:\n",
    "        for train_index, test_index in splitter.split(X, Y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            # model fitting\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # prediction\n",
    "            predict = model.predict(X_test)\n",
    "\n",
    "            accuracy_temp = metrics.accuracy_score(y_test, predict)\n",
    "            precision_temp = metrics.precision_score(y_test, predict, average=\"micro\")\n",
    "            recall_temp = metrics.recall_score(y_test, predict, average=\"micro\")\n",
    "            f1_temp = metrics.f1_score(y_test, predict, average=\"micro\")\n",
    "            hamming_loss = metrics.hamming_loss(y_test, predict)\n",
    "\n",
    "            accuracy = accuracy + accuracy_temp\n",
    "            precision = precision + precision_temp\n",
    "            recall = recall+ recall_temp\n",
    "            f1= f1 + f1_temp\n",
    "\n",
    "\n",
    "    #         # evaluation scores\n",
    "    #         explained_variance_score_temp = explained_variance_score(y_test, predict)\n",
    "    #         mean_absolute_error_temp = mean_absolute_error(y_test, predict)\n",
    "    #         mean_squared_error_temp = mean_squared_error(y_test, predict)\n",
    "    #         mean_squared_log_error_temp = mean_squared_log_error(y_test, predict)\n",
    "    #         median_absolute_error_temp = median_absolute_error(y_test, predict)\n",
    "    #         r2_score_temp = r2_score(y_test, predict)\n",
    "\n",
    "    #         explained_variance_score_val = explained_variance_score_val + explained_variance_score_temp\n",
    "    #         mean_absolute_error_val = mean_absolute_error_val + mean_absolute_error_temp\n",
    "    #         mean_squared_error_val = mean_squared_error_val + mean_squared_error_temp\n",
    "    #         mean_squared_log_error_val = mean_squared_log_error_val + mean_squared_log_error_temp\n",
    "    #         median_absolute_error_val = median_absolute_error_val + median_absolute_error_temp\n",
    "    #         r2_score_val = r2_score_val + r2_score_temp\n",
    "\n",
    "            if details:\n",
    "                print(\"*\"*25,  \" ITERATION - \", i+1, \"*\"*25)\n",
    "                #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                print(\"accuracy_score\", metrics.accuracy_score(y_test, predict))\n",
    "                print(\"precision_score\", metrics.precision_score(y_test, predict, average=\"micro\"))\n",
    "                print(\"recall_score\", metrics.recall_score(y_test, predict, average=\"micro\"))\n",
    "                print(\"f1_score\", metrics.f1_score(y_test, predict, average=\"micro\"))\n",
    "                print(\"hamming_loss\", metrics.hamming_loss(y_test, predict))\n",
    "            #     precision, recall, thresholds = metrics.precision_recall_curve(y_test, predict)\n",
    "            #     print(\"average_precision_score\", metrics.average_precision_score(y_test, predict, average=\"micro\"))\n",
    "            #     print(\"fbeta_score\", metrics.fbeta_score(y_test, predict))\n",
    "            #     print(\"roc_auc_score\", metrics.roc_auc_score(y_test, predict, average=\"micro\"))\n",
    "\n",
    "    #             print(\"-\"*35)\n",
    "    #             print(metrics.classification_report(y_test, predict))\n",
    "    #             print(\"-\"*35)\n",
    "    #             print(\"confusion Matrix:\\n\\n\", metrics.confusion_matrix(y_test, predict))\n",
    "    #             print(\"-\"*35)\n",
    "                print(\"\\n\")\n",
    "\n",
    "            i+=1\n",
    "\n",
    "\n",
    "        split_num = splitter.get_n_splits()\n",
    "        accuracy = accuracy/split_num\n",
    "        precision = precision/split_num\n",
    "        recall = recall/split_num\n",
    "        f1 = f1/split_num\n",
    "\n",
    "    else:\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        # prediction\n",
    "        predict = model.predict(X_test)\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y_test, predict)\n",
    "        precision = metrics.precision_score(y_test, predict, average=\"micro\")\n",
    "        recall = metrics.recall_score(y_test, predict, average=\"micro\")\n",
    "        f1 = metrics.f1_score(y_test, predict, average=\"micro\")\n",
    "        hamming_loss = metrics.hamming_loss(y_test, predict)\n",
    "    if report:\n",
    "        if splitter:\n",
    "            print(\"*\"*50, \" Average For\", i+1, \" Folds\", \"*\"*50)\n",
    "        print(\"\\n\")\n",
    "        print(\"Average Accuracy Score: \", accuracy)\n",
    "        print(\"Average pPrecision Score: \", precision)\n",
    "        print(\"Average Recall Score: \", recall)\n",
    "        print(\"Average F1 Score:\", f1)\n",
    "\n",
    "#         print('%50s%s' % (\"Average explained_variance_score: \", explained_variance_score_val))\n",
    "#         print('%50s%s' % (\"Average mean_absolute_error: \", mean_absolute_error_val))\n",
    "#         print('%50s%s' % (\"Average mean_squared_error: \", mean_squared_error_val))\n",
    "#         print('%50s%s' % (\"Average mean_squared_log_error: \", mean_squared_log_error_val))\n",
    "#         print('%50s%s' % (\"Average median_absolute_error: \", median_absolute_error_val))\n",
    "#         print('%50s%s' % (\"Average r2_score: \", r2_score_val))\n",
    "#         print(\"\\n\")\n",
    "#         print(\"*\"*100)\n",
    "# #         print(\"*\"*50, \" END \", \"*\"*50)\n",
    "    \n",
    "#     return explained_variance_score_val, mean_absolute_error_val, mean_squared_error_val, mean_squared_log_error_val, median_absolute_error_val, r2_score_val\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n",
      "Error: 159571\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "\n",
    "features = feature_word_string(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n"
     ]
    }
   ],
   "source": [
    "print(len(features))\n",
    "features = np.array(features, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliter Description:\n",
      "StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=0.5,\n",
      "            train_size=None)\n",
      "159571\n",
      "comment Type:  any \n",
      "\n",
      "\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anjana Tiha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class RandomizedLogisticRegression is deprecated; The class RandomizedLogisticRegression is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.9012759130674555\n",
      "Average pPrecision Score:  0.9012759130674555\n",
      "Average Recall Score:  0.9012759130674555\n",
      "Average F1 Score: 0.9012759130674555\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.8851201965257063\n",
      "Average pPrecision Score:  0.8851201965257063\n",
      "Average Recall Score:  0.8851201965257063\n",
      "Average F1 Score: 0.8851201965257063\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.8369388113202817\n",
      "Average pPrecision Score:  0.8369388113202817\n",
      "Average Recall Score:  0.8369388113202817\n",
      "Average F1 Score: 0.8369388113202817\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.8935778206702929\n",
      "Average pPrecision Score:  0.8935778206702929\n",
      "Average Recall Score:  0.8935778206702929\n",
      "Average F1 Score: 0.8935778206702931\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.9034116260998172\n",
      "Average pPrecision Score:  0.9034116260998172\n",
      "Average Recall Score:  0.9034116260998172\n",
      "Average F1 Score: 0.9034116260998172\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.8998145037976588\n",
      "Average pPrecision Score:  0.8998145037976588\n",
      "Average Recall Score:  0.8998145037976588\n",
      "Average F1 Score: 0.8998145037976588\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.9006968641114984\n",
      "Average pPrecision Score:  0.9006968641114984\n",
      "Average Recall Score:  0.9006968641114984\n",
      "Average F1 Score: 0.9006968641114984\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.8999874664728147\n",
      "Average pPrecision Score:  0.8999874664728147\n",
      "Average Recall Score:  0.8999874664728147\n",
      "Average F1 Score: 0.8999874664728147\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.8659865139247488\n",
      "Average pPrecision Score:  0.8659865139247488\n",
      "Average Recall Score:  0.8659865139247488\n",
      "Average F1 Score: 0.8659865139247488\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
      "               store_covariance=False, store_covariances=None, tol=0.0001)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anjana Tiha\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.8883789135938637\n",
      "Average pPrecision Score:  0.8883789135938637\n",
      "Average Recall Score:  0.8883789135938637\n",
      "Average F1 Score: 0.8883789135938637\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.8952573133131125\n",
      "Average pPrecision Score:  0.8952573133131125\n",
      "Average Recall Score:  0.8952573133131125\n",
      "Average F1 Score: 0.8952573133131125\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# classifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import (AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomTreesEmbedding, RandomForestClassifier, VotingClassifier)\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB \n",
    "from sklearn.neighbors import KDTree, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# regressor\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import (AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomTreesEmbedding, RandomForestRegressor, VotingClassifier)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ARDRegression, LinearRegression, LogisticRegression, LogisticRegressionCV, logistic_regression_path, HuberRegressor, PassiveAggressiveRegressor, RandomizedLogisticRegression, RANSACRegressor, SGDRegressor, TheilSenRegressor\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB \n",
    "from sklearn.neighbors import KDTree, KNeighborsRegressor, NearestNeighbors, RadiusNeighborsRegressor\n",
    "from sklearn.neural_network import BernoulliRBM, MLPRegressor\n",
    "from sklearn.svm import LinearSVR, NuSVR, SVR\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "# kf = KFold(n_splits = 5, random_state=None, shuffle =True)\n",
    "\n",
    "classifiers = {\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "#     \"BernoulliRBM\": BernoulliRBM(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"ExtraTreesClassifier\": ExtraTreesClassifier(),\n",
    "#     \"GaussianMixture\": GaussianMixture(),\n",
    "#     \"GaussianNB\": GaussianNB(),\n",
    "#     \"GaussianProcessClassifier\": GaussianProcessClassifier(),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "#     \"KDTree\": KDTree(),\n",
    "#     \"KNeighborsClassifier\": KNeighborsClassifier(3),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"MLPClassifier\": MLPClassifier(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "#     \"NearestNeighbors\": NearestNeighbors(),\n",
    "#     \"NuSVC\": NuSVC(),\n",
    "    \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025),\n",
    "    \"SVC\": SVC(),\n",
    "    \"SVC Gamma\": SVC(gamma=2, C=1)\n",
    "#     VotingClassifier: VotingClassifier(),\n",
    "}\n",
    "\n",
    "classifiers2 = {\n",
    "#     \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "#     \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "#     \"KNeighborsClassifier\": KNeighborsClassifier(3),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "#     \"NuSVC\": NuSVC(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025)\n",
    "#     \"SVC\": SVC(),\n",
    "#     \"SVC Gamma\": SVC(gamma=2, C=1)\n",
    "}\n",
    "\n",
    "classifiers3 = {\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025)\n",
    "}\n",
    "\n",
    "\n",
    "regressors = {\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(),\n",
    "#     \"ARDRegression\": ARDRegression(),\n",
    "    \"BaggingRegressor\": BaggingRegressor(),\n",
    "#     \"BernoulliRBM\": BernoulliRBM(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(),\n",
    "    \"ExtraTreeRegressor\": ExtraTreeRegressor(),\n",
    "#     \"GaussianMixture\": GaussianMixture(),\n",
    "#     \"GaussianNB\": GaussianNB(),\n",
    "    \"GaussianProcessRegressor\": GaussianProcessRegressor(),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n",
    "    \"HuberRegressor\": HuberRegressor(),\n",
    "#     \"IsotonicRegression\": IsotonicRegression(),\n",
    "    \"KernelRidge\": KernelRidge(),\n",
    "#     \"KDTree\": KDTree(),\n",
    "#     \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "#     \"LinearRegression\": LinearRegression(), \n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LogisticRegressionCV\": LogisticRegressionCV(),\n",
    "#     \"logistic_regression_path\": logistic_regression_path(),\n",
    "    \"LinearSVR\": LinearSVR(),\n",
    "    \"MLPRegressor\": MLPRegressor(),\n",
    "#     \"MultinomialNB\": MultinomialNB(),\n",
    "    \"NuSVR\": NuSVR(),\n",
    "    \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(),\n",
    "#     \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "    \"RadiusNeighborsRegressor\": RadiusNeighborsRegressor(),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "    \"RandomizedLogisticRegression\": RandomizedLogisticRegression(),\n",
    "    \"RANSACRegressor\": RANSACRegressor(),\n",
    "    \"SGDRegressor\": SGDRegressor(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"TheilSenRegressor\": TheilSenRegressor(),\n",
    "}\n",
    "\n",
    "\n",
    "    \n",
    "# splitter = kf \n",
    "splitter = sss\n",
    "# splitter = None\n",
    "report = 1\n",
    "details = 1\n",
    "evaluation = {}\n",
    "\n",
    "print(\"Spliter Description:\")\n",
    "print(splitter)\n",
    "     \n",
    "columns2 =['any'] \n",
    "\n",
    "print(len(train))\n",
    "\n",
    "X_features = features\n",
    "\n",
    "for comment_type in columns2:\n",
    "    print(\"comment Type: \", comment_type, \"\\n\\n\")\n",
    "    for name in classifiers:\n",
    "        evaluation_temp = []\n",
    "        accuracy, precision, recall, f1 = model_evaluation(X_features, train[comment_type], None, None, splitter, classifiers[name], report, details=None)\n",
    "#         accuracy, precision, recall, f1 = model_evaluation(X, train[comment_type], test_x, test_labels[comment_type], splitter, classifiers[name], report, details=None)\n",
    "        evaluation_temp.append(accuracy)\n",
    "        evaluation_temp.append(precision)\n",
    "        evaluation_temp.append(recall)\n",
    "        evaluation_temp.append(f1)\n",
    "        evaluation[name] = evaluation_temp\n",
    "        gc.collect()\n",
    "    rows_list = []\n",
    "    for name in evaluation:\n",
    "        rows_list.append([name]+evaluation[name])\n",
    "                           \n",
    "    evaluation_pd = pd.DataFrame(rows_list, columns=['model', 'accuracy', 'precision', 'recall', 'f1']) \n",
    "#     print(evaluation_pd)\n",
    "\n",
    "\n",
    "            \n",
    "# for name in regressors:\n",
    "#     evaluation_temp = []\n",
    "    \n",
    "#     explained_variance_score_val, mean_absolute_error_val, mean_squared_error_val, mean_squared_log_error_val, median_absolute_error_val, r2_score_val = model_evaluation(X, Y, splitter, regressors[name], report, details=None)\n",
    "#     evaluation_temp.append(explained_variance_score_val)\n",
    "#     evaluation_temp.append(mean_absolute_error_val)\n",
    "#     evaluation_temp.append(mean_squared_error_val)\n",
    "#     evaluation_temp.append(mean_squared_log_error_val)\n",
    "#     evaluation_temp.append(median_absolute_error_val)\n",
    "#     evaluation_temp.append(r2_score_val)\n",
    "#     evaluation[name] = evaluation_temp\n",
    "    \n",
    "\n",
    "# rows_list = []\n",
    "# for name in evaluation:\n",
    "#     rows_list.append([name]+evaluation[name])\n",
    "                           \n",
    "# evaluation_pd = pd.DataFrame(rows_list, columns=['explained_variance_score',  'mean_absolute_error', 'mean_squared_error', 'mean_squared_log_error', 'median_absolute_error', 'r2_score']) \n",
    "# evaluation_pd = pd.DataFrame(rows_list, columns=['model', 'accuracy', 'precision', 'recall', 'f1']) \n",
    "# evaluation_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "figure(num=None, figsize=(14, 6), dpi=250)\n",
    "\n",
    "labels= ['accuracy', 'precision', 'recall', 'f1']\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for n in range(0,4):\n",
    "    plt.plot([name for name in evaluation],[evaluation[name][n] for name in evaluation], label = labels[n])\n",
    "\n",
    "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=True, fancybox=True)\n",
    "plt.xticks(rotation=45)\n",
    "# leg.get_frame().set_alpha(0.5)\n",
    "plt.legend()\n",
    "ax.tick_params(labelsize='large', width=5)\n",
    "ax.grid(True, linestyle='-.')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlabel('x label')\n",
    "plt.ylabel('y label')\n",
    "\n",
    "plt.title(\"TITLE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
