{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk, re, time\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from tokenize import tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\Anjana Tiha\\Drive D\\Programming\\Projects\\Toxic Comment Classification Challenge\\Toxic Comment Classification Challenge\\all\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\Anjana Tiha\\Drive D\\Programming\\Projects\\Toxic Comment Classification Challenge\\Toxic Comment Classification Challenge\\all\\test.csv')\n",
    "test_labels = pd.read_csv(r'C:\\Users\\Anjana Tiha\\Drive D\\Programming\\Projects\\Toxic Comment Classification Challenge\\Toxic Comment Classification Challenge\\all\\test_labels.csv')\n",
    "sample_submission = pd.read_csv(r'C:\\Users\\Anjana Tiha\\Drive D\\Programming\\Projects\\Toxic Comment Classification Challenge\\Toxic Comment Classification Challenge\\all\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['none'] = 1-train[columns].max(axis=1)\n",
    "test_labels['none'] = 1-test_labels[columns].max(axis=1)\n",
    "train['any'] = train[columns].max(axis=1)\n",
    "test_labels['any'] = test_labels[columns].max(axis=1)\n",
    "train['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "test['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'none', 'any']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True):\n",
    "    cached_stopwords = stopwords.words(\"english\")\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = ' '.join([word for word in text.split() if word not in cached_stopwords])\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return text \n",
    "\n",
    "\n",
    "def preprocess(text, remove_stopwords=True):\n",
    "#     cached_stopwords = stopwords.words(\"english\")\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "#     text = ' '.join([word for word in text.split() if word not in cached_stopwords])\n",
    "#     text = ''.join([c for c in text if c not in punctuation])\n",
    "#     text = text.strip()\n",
    "#     text = text.split()\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize(text):\n",
    "#     cached_stopwords = stopwords.words(\"english\")\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "#     text = re.sub(r\" +\", \" \", text)\n",
    "#     text = ' '.join([word for word in text.split() if word not in cached_stopwords])\n",
    "#     text = ''.join([c for c in text if c not in punctuation])\n",
    "#     text = text.strip()\n",
    "#     text = text.split()\n",
    "    return text.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=clean_text, min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1, smooth_idf=1 )\n",
    "\n",
    "# tfidf = TfidfVectorizer(tokenizer=clean_text, min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1, smooth_idf=1 )\n",
    "# tfidf = TfidfVectorizer(decode_error='strict', strip_accents='unicode', lowercase=True, preprocessor=preprocess, tokenizer=tokenize, analyzer='word', stop_words='english', ngram_range=(1, 1), max_df=1.0, min_df=1, norm='l2', use_idf=True, smooth_idf=True)\n",
    "\n",
    "# tfidf = TfidfVectorizer(decode_error='strict', strip_accents='unicode', lowercase=True, preprocessor=preprocess, tokenizer=tokenize, analyzer='word', stop_words='english', max_df=0.9, min_df=3, norm='l2', use_idf=True, smooth_idf=True)\n",
    "\n",
    "x = tfidf.fit_transform(train['comment_text'])\n",
    "test_x = tfidf.transform(test['comment_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(data.isnull().sum(axis=1))\n",
    "# X = features.values\n",
    "# Y = target\n",
    "# X = x+test_x\n",
    "X = x\n",
    "# Y = train['comment_text'] + test['comment_text']\n",
    "# Y = train['comment_text']\n",
    "\n",
    "# # print(features.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "# from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score\n",
    "    \n",
    "def model_evaluation(X, Y, X_test, y_test, splitter, model, report, details):\n",
    "    accuracy = 0\n",
    "    f1 = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    i=0\n",
    "    if report:\n",
    "        print(\"*\"*50, \" START \", \"*\"*50)\n",
    "#         print(\"Spliter Description:\")\n",
    "#         print(splitter)\n",
    "#         print(\"-\"*100, \"\\n\")\n",
    "        print(\"Model Description:\")\n",
    "        print(model)\n",
    "        print(\"-\"*100,\"\\n\")\n",
    "      \n",
    "    if splitter:\n",
    "        for train_index, test_index in splitter.split(X, Y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            # model fitting\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # prediction\n",
    "            predict = model.predict(X_test)\n",
    "\n",
    "            accuracy_temp = metrics.accuracy_score(y_test, predict)\n",
    "            precision_temp = metrics.precision_score(y_test, predict, average=\"micro\")\n",
    "            recall_temp = metrics.recall_score(y_test, predict, average=\"micro\")\n",
    "            f1_temp = metrics.f1_score(y_test, predict, average=\"micro\")\n",
    "            hamming_loss = metrics.hamming_loss(y_test, predict)\n",
    "\n",
    "            accuracy = accuracy + accuracy_temp\n",
    "            precision = precision + precision_temp\n",
    "            recall = recall+ recall_temp\n",
    "            f1= f1 + f1_temp\n",
    "\n",
    "\n",
    "    #         # evaluation scores\n",
    "    #         explained_variance_score_temp = explained_variance_score(y_test, predict)\n",
    "    #         mean_absolute_error_temp = mean_absolute_error(y_test, predict)\n",
    "    #         mean_squared_error_temp = mean_squared_error(y_test, predict)\n",
    "    #         mean_squared_log_error_temp = mean_squared_log_error(y_test, predict)\n",
    "    #         median_absolute_error_temp = median_absolute_error(y_test, predict)\n",
    "    #         r2_score_temp = r2_score(y_test, predict)\n",
    "\n",
    "    #         explained_variance_score_val = explained_variance_score_val + explained_variance_score_temp\n",
    "    #         mean_absolute_error_val = mean_absolute_error_val + mean_absolute_error_temp\n",
    "    #         mean_squared_error_val = mean_squared_error_val + mean_squared_error_temp\n",
    "    #         mean_squared_log_error_val = mean_squared_log_error_val + mean_squared_log_error_temp\n",
    "    #         median_absolute_error_val = median_absolute_error_val + median_absolute_error_temp\n",
    "    #         r2_score_val = r2_score_val + r2_score_temp\n",
    "\n",
    "            if details:\n",
    "                print(\"*\"*25,  \" ITERATION - \", i+1, \"*\"*25)\n",
    "                #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                print(\"accuracy_score\", metrics.accuracy_score(y_test, predict))\n",
    "                print(\"precision_score\", metrics.precision_score(y_test, predict, average=\"micro\"))\n",
    "                print(\"recall_score\", metrics.recall_score(y_test, predict, average=\"micro\"))\n",
    "                print(\"f1_score\", metrics.f1_score(y_test, predict, average=\"micro\"))\n",
    "                print(\"hamming_loss\", metrics.hamming_loss(y_test, predict))\n",
    "            #     precision, recall, thresholds = metrics.precision_recall_curve(y_test, predict)\n",
    "            #     print(\"average_precision_score\", metrics.average_precision_score(y_test, predict, average=\"micro\"))\n",
    "            #     print(\"fbeta_score\", metrics.fbeta_score(y_test, predict))\n",
    "            #     print(\"roc_auc_score\", metrics.roc_auc_score(y_test, predict, average=\"micro\"))\n",
    "\n",
    "    #             print(\"-\"*35)\n",
    "    #             print(metrics.classification_report(y_test, predict))\n",
    "    #             print(\"-\"*35)\n",
    "    #             print(\"confusion Matrix:\\n\\n\", metrics.confusion_matrix(y_test, predict))\n",
    "    #             print(\"-\"*35)\n",
    "                print(\"\\n\")\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        split_num = splitter.get_n_splits()\n",
    "\n",
    "        split_num = splitter.get_n_splits()\n",
    "        accuracy = accuracy/split_num\n",
    "        precision = precision/split_num\n",
    "        recall = recall/split_num\n",
    "        f1 = f1/split_num\n",
    "\n",
    "    else:\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        # prediction\n",
    "        predict = model.predict(X_test)\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y_test, predict)\n",
    "        precision = metrics.precision_score(y_test, predict, average=\"micro\")\n",
    "        recall = metrics.recall_score(y_test, predict, average=\"micro\")\n",
    "        f1 = metrics.f1_score(y_test, predict, average=\"micro\")\n",
    "        hamming_loss = metrics.hamming_loss(y_test, predict)\n",
    "    if report:\n",
    "        if splitter:\n",
    "            print(\"*\"*50, \" Average For\", i+1, \" Folds\", \"*\"*50)\n",
    "        print(\"\\n\")\n",
    "        print(\"Average Accuracy Score: \", accuracy)\n",
    "        print(\"Average pPrecision Score: \", precision)\n",
    "        print(\"Average Recall Score: \", recall)\n",
    "        print(\"Average F1 Score:\", f1)\n",
    "\n",
    "#         print('%50s%s' % (\"Average explained_variance_score: \", explained_variance_score_val))\n",
    "#         print('%50s%s' % (\"Average mean_absolute_error: \", mean_absolute_error_val))\n",
    "#         print('%50s%s' % (\"Average mean_squared_error: \", mean_squared_error_val))\n",
    "#         print('%50s%s' % (\"Average mean_squared_log_error: \", mean_squared_log_error_val))\n",
    "#         print('%50s%s' % (\"Average median_absolute_error: \", median_absolute_error_val))\n",
    "#         print('%50s%s' % (\"Average r2_score: \", r2_score_val))\n",
    "#         print(\"\\n\")\n",
    "#         print(\"*\"*100)\n",
    "# #         print(\"*\"*50, \" END \", \"*\"*50)\n",
    "    \n",
    "#     return explained_variance_score_val, mean_absolute_error_val, mean_squared_error_val, mean_squared_log_error_val, median_absolute_error_val, r2_score_val\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anjana Tiha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class RandomizedLogisticRegression is deprecated; The class RandomizedLogisticRegression is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliter Description:\n",
      "None\n",
      "comment Type:  any \n",
      "\n",
      "\n",
      "**************************************************  START  **************************************************\n",
      "Model Description:\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "\n",
      "Average Accuracy Score:  0.37436995638661824\n",
      "Average pPrecision Score:  0.37436995638661824\n",
      "Average Recall Score:  0.37436995638661824\n",
      "Average F1 Score: 0.37436995638661824\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'GaussianNB'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-895a92160419>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclassifiers3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mevaluation_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcomment_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcomment_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0mevaluation_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mevaluation_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'GaussianNB'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# classifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import (AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomTreesEmbedding, RandomForestClassifier, VotingClassifier)\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB \n",
    "from sklearn.neighbors import KDTree, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# regressor\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import (AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomTreesEmbedding, RandomForestRegressor, VotingClassifier)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ARDRegression, LinearRegression, LogisticRegression, LogisticRegressionCV, logistic_regression_path, HuberRegressor, PassiveAggressiveRegressor, RandomizedLogisticRegression, RANSACRegressor, SGDRegressor, TheilSenRegressor\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB \n",
    "from sklearn.neighbors import KDTree, KNeighborsRegressor, NearestNeighbors, RadiusNeighborsRegressor\n",
    "from sklearn.neural_network import BernoulliRBM, MLPRegressor\n",
    "from sklearn.svm import LinearSVR, NuSVR, SVR\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "# sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "# kf = KFold(n_splits = 5, random_state=None, shuffle =True)\n",
    "\n",
    "classifiers = {\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "#     \"BernoulliRBM\": BernoulliRBM(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"ExtraTreesClassifier\": ExtraTreesClassifier(),\n",
    "#     \"GaussianMixture\": GaussianMixture(),\n",
    "#     \"GaussianNB\": GaussianNB(),\n",
    "#     \"GaussianProcessClassifier\": GaussianProcessClassifier(),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "#     \"KDTree\": KDTree(),\n",
    "#     \"KNeighborsClassifier\": KNeighborsClassifier(3),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"MLPClassifier\": MLPClassifier(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "#     \"NearestNeighbors\": NearestNeighbors(),\n",
    "#     \"NuSVC\": NuSVC(),\n",
    "    \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025),\n",
    "    \"SVC\": SVC(),\n",
    "    \"SVC Gamma\": SVC(gamma=2, C=1)\n",
    "#     VotingClassifier: VotingClassifier(),\n",
    "}\n",
    "\n",
    "classifiers2 = {\n",
    "#     \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "#     \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "#     \"KNeighborsClassifier\": KNeighborsClassifier(3),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "#     \"NuSVC\": NuSVC(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025)\n",
    "#     \"SVC\": SVC(),\n",
    "#     \"SVC Gamma\": SVC(gamma=2, C=1)\n",
    "}\n",
    "\n",
    "classifiers3 = {\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025)\n",
    "}\n",
    "\n",
    "\n",
    "regressors = {\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(),\n",
    "#     \"ARDRegression\": ARDRegression(),\n",
    "    \"BaggingRegressor\": BaggingRegressor(),\n",
    "#     \"BernoulliRBM\": BernoulliRBM(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(),\n",
    "    \"ExtraTreeRegressor\": ExtraTreeRegressor(),\n",
    "#     \"GaussianMixture\": GaussianMixture(),\n",
    "#     \"GaussianNB\": GaussianNB(),\n",
    "    \"GaussianProcessRegressor\": GaussianProcessRegressor(),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n",
    "    \"HuberRegressor\": HuberRegressor(),\n",
    "#     \"IsotonicRegression\": IsotonicRegression(),\n",
    "    \"KernelRidge\": KernelRidge(),\n",
    "#     \"KDTree\": KDTree(),\n",
    "#     \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "#     \"LinearRegression\": LinearRegression(), \n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LogisticRegressionCV\": LogisticRegressionCV(),\n",
    "#     \"logistic_regression_path\": logistic_regression_path(),\n",
    "    \"LinearSVR\": LinearSVR(),\n",
    "    \"MLPRegressor\": MLPRegressor(),\n",
    "#     \"MultinomialNB\": MultinomialNB(),\n",
    "    \"NuSVR\": NuSVR(),\n",
    "    \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(),\n",
    "#     \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "    \"RadiusNeighborsRegressor\": RadiusNeighborsRegressor(),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "    \"RandomizedLogisticRegression\": RandomizedLogisticRegression(),\n",
    "    \"RANSACRegressor\": RANSACRegressor(),\n",
    "    \"SGDRegressor\": SGDRegressor(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"TheilSenRegressor\": TheilSenRegressor(),\n",
    "}\n",
    "\n",
    "\n",
    "    \n",
    "# splitter = kf \n",
    "# splitter = sss\n",
    "splitter = None\n",
    "report = 1\n",
    "details = 1\n",
    "evaluation = {}\n",
    "\n",
    "print(\"Spliter Description:\")\n",
    "print(splitter)\n",
    "     \n",
    "columns2 =['any'] \n",
    "\n",
    "for comment_type in columns2:\n",
    "    print(\"comment Type: \", comment_type, \"\\n\\n\")\n",
    "    for name in classifiers3:\n",
    "        evaluation_temp = []\n",
    "        accuracy, precision, recall, f1 = model_evaluation(X, train[comment_type], test_x, test_labels[comment_type], splitter, classifiers[name], report, details=None)\n",
    "        evaluation_temp.append(accuracy)\n",
    "        evaluation_temp.append(precision)\n",
    "        evaluation_temp.append(recall)\n",
    "        evaluation_temp.append(f1)\n",
    "        evaluation[name] = evaluation_temp\n",
    "        gc.collect()\n",
    "    rows_list = []\n",
    "    for name in evaluation:\n",
    "        rows_list.append([name]+evaluation[name])\n",
    "                           \n",
    "    evaluation_pd = pd.DataFrame(rows_list, columns=['model', 'accuracy', 'precision', 'recall', 'f1']) \n",
    "#     print(evaluation_pd)\n",
    "\n",
    "\n",
    "            \n",
    "# for name in regressors:\n",
    "#     evaluation_temp = []\n",
    "    \n",
    "#     explained_variance_score_val, mean_absolute_error_val, mean_squared_error_val, mean_squared_log_error_val, median_absolute_error_val, r2_score_val = model_evaluation(X, Y, splitter, regressors[name], report, details=None)\n",
    "#     evaluation_temp.append(explained_variance_score_val)\n",
    "#     evaluation_temp.append(mean_absolute_error_val)\n",
    "#     evaluation_temp.append(mean_squared_error_val)\n",
    "#     evaluation_temp.append(mean_squared_log_error_val)\n",
    "#     evaluation_temp.append(median_absolute_error_val)\n",
    "#     evaluation_temp.append(r2_score_val)\n",
    "#     evaluation[name] = evaluation_temp\n",
    "    \n",
    "\n",
    "# rows_list = []\n",
    "# for name in evaluation:\n",
    "#     rows_list.append([name]+evaluation[name])\n",
    "                           \n",
    "# evaluation_pd = pd.DataFrame(rows_list, columns=['explained_variance_score',  'mean_absolute_error', 'mean_squared_error', 'mean_squared_log_error', 'median_absolute_error', 'r2_score']) \n",
    "# evaluation_pd = pd.DataFrame(rows_list, columns=['model', 'accuracy', 'precision', 'recall', 'f1']) \n",
    "# evaluation_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "figure(num=None, figsize=(14, 6), dpi=250)\n",
    "\n",
    "labels= ['accuracy', 'precision', 'recall', 'f1']\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for n in range(0,4):\n",
    "    plt.plot([name for name in evaluation],[evaluation[name][n] for name in evaluation], label = labels[n])\n",
    "\n",
    "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=True, fancybox=True)\n",
    "plt.xticks(rotation=45)\n",
    "# leg.get_frame().set_alpha(0.5)\n",
    "plt.legend()\n",
    "ax.tick_params(labelsize='large', width=5)\n",
    "ax.grid(True, linestyle='-.')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlabel('x label')\n",
    "plt.ylabel('y label')\n",
    "\n",
    "plt.title(\"TITLE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
